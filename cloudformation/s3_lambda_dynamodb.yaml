AWSTemplateFormatVersion: 2010-09-09
Parameters:
  BucketName:
    Default: ""
    Type: String
Conditions:
  CreateBucket: !Not 
    - !Equals 
      - !Ref BucketName
      - ""
Resources:
  DDBTable:
    Type: 'AWS::DynamoDB::Table'
    Properties:
      AttributeDefinitions:
        - AttributeName: title
          AttributeType: S
        - AttributeName: year
          AttributeType: 'N'
      KeySchema:
        - AttributeName: year
          KeyType: HASH
        - AttributeName: title
          KeyType: RANGE
      ProvisionedThroughput:
        ReadCapacityUnits: 10
        WriteCapacityUnits: 10
      TableName: "movies"
  S3fordynamo:
    Type: 'AWS::S3::Bucket'
    Condition: CreateBucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Ref BucketName
  ddbinputtransform:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: "ddb_input_transform"
      Handler: index.lambda_handler
      Role:
        Fn::ImportValue: S3DynamoArn
      Architectures:
        - x86_64
      Runtime: python3.9
      Timeout: 40
      MemorySize: 1024
      Code:
        ZipFile: |
          def lambda_handler(event, context):
            print('hello from lambda')
    DependsOn:
      - "DDBTable"
  batchwrites3dynamo:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: "batch_write_s3_dynamodb"
      Handler: index.lambda_handler
      Role:
        Fn::ImportValue: S3DynamoArn
      Architectures:
        - x86_64
      Runtime: python3.9
      Timeout: 300
      MemorySize: 1024
      Code:
        ZipFile: |
          def lambda_handler(event, context):
            print('hello from lambda')
    DependsOn:
      - "DDBTable"
  SNSSubscriptionQueue:
    Type: 'AWS::SNS::Subscription'
    Properties:
      TopicArn:
        Fn::ImportValue: SNSTopics-etl
      Endpoint:
        Fn::ImportValue: SQS-etl
      Protocol: sqs
      RawMessageDelivery: 'true'
