{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1f7ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1656267673903_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-34-77-83.ec2.internal:20888/proxy/application_1656267673903_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-34-112-35.ec2.internal:8042/node/containerlogs/container_1656267673903_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue import DynamicFrame\n",
    "import boto3\n",
    "import os \n",
    "\n",
    "# uncomment this for debugging and experimenting\n",
    "\n",
    "args = {}\n",
    "args[\"fraud_samples\"] = 12\n",
    "args[\"legit_samples\"] = 130\n",
    "args[\"bucket\"] = \"fraud-sample-data\"\n",
    "args[\"train-source-key\"] = \"input/fraudTrain.csv\"\n",
    "args[\"test-source-key\"] = \"input/fraudTest.csv\"\n",
    "args[\"train-dest-key\"] = \"input/fraudTrain_glue_transformed.csv\"\n",
    "args[\"test-dest-key\"]=  \"input/fraudTest_glue_transformed.csv\"\n",
    "args[\"train_max_cut_off\"]= \"2020-04-30 00:00:00\"\n",
    "args[\"test_min_cut_off\"]= \"2020-08-30 00:00:00\"\n",
    "args[\"catalog_db\"] =  \"default\"\n",
    "args[\"catalog_table\"] = \"fraud-raw-input\"\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# uncomment this for running job in glue\n",
    "\n",
    "#job = Job(glueContext)\n",
    "#args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"train-source\", \"test-source\", \"train-dest\",\"test-dest\", \"train_max_cut_off\", \"test_min_cut_off\"])\n",
    "#job.init(args[\"JOB_NAME\"], args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af70c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def sparkUnion(glueContext, unionType, mapping, transformation_ctx) -> DynamicFrame:\n",
    "    for alias, frame in mapping.items():\n",
    "        frame.toDF().createOrReplaceTempView(alias)\n",
    "    result = spark.sql(\n",
    "        \"(select * from source1) UNION \" + unionType + \" (select * from source2)\"\n",
    "    )\n",
    "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
    "\n",
    "\n",
    "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n",
    "    for alias, frame in mapping.items():\n",
    "        frame.toDF().createOrReplaceTempView(alias)\n",
    "    result = spark.sql(query)\n",
    "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
    "\n",
    "\n",
    "def write_output_to_s3(dyf, s3_path, prefix, renamed_key, transformation_ctx):\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    resource = boto3.resource('s3')\n",
    "\n",
    "    print(f\"saving dyanmic frame to S3 bucket with prefix path: {prefix}\")\n",
    "    # Script generated for node S3 bucket\n",
    "    S3bucket_dyf = glueContext.write_dynamic_frame.from_options(\n",
    "        frame=dyf ,\n",
    "        connection_type=\"s3\",\n",
    "        format=\"csv\",\n",
    "        connection_options={\"path\": s3_path},\n",
    "        transformation_ctx=transformation_ctx,\n",
    "    )\n",
    "\n",
    "    #https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.copy\n",
    "    response = client.list_objects(\n",
    "        Bucket=args['bucket'],\n",
    "        Prefix=f\"{prefix}/run-\",\n",
    "    )\n",
    "\n",
    "    objectkey_to_rename = response['Contents'][0]['Key']\n",
    "\n",
    "    copy_output = {\n",
    "        'Bucket': args['bucket'],\n",
    "        'Key': objectkey_to_rename\n",
    "    }\n",
    "\n",
    "    print(f\"renaming filename to {renamed_key} as glue output filename is random\")\n",
    "\n",
    "    resource.meta.client.copy(copy_output, args['bucket'], renamed_key)\n",
    "    print(f\"deleting original output {objectkey_to_rename}....\")\n",
    "    response = client.delete_object(\n",
    "        Bucket=args['bucket'],\n",
    "        Key=objectkey_to_rename \n",
    "    )\n",
    "    \n",
    "# set these but they should not overlap\n",
    "TRAIN_MAX_TIMESTAMP = args[\"train_max_cut_off\"]\n",
    "TEST_MIN_TIMESTAMP = args[\"test_min_cut_off\"]\n",
    "\n",
    "fraud_samples = args[\"fraud_samples\"]\n",
    "legit_samples = args[\"legit_samples\"]\n",
    "\n",
    "\n",
    "# Script generated for node SQL\n",
    "SqlQuery0 = f'''\n",
    "select * from \n",
    "(\n",
    "    (\n",
    "    select * from myDataSource\n",
    "    where EVENT_LABEL == 'fraud'\n",
    "    order BY RAND() \n",
    "    limit {fraud_samples}\n",
    "    ) \n",
    "    union all\n",
    "    (\n",
    "    select * from myDataSource\n",
    "    where EVENT_LABEL == 'legit'\n",
    "    order BY RAND() \n",
    "    limit {legit_samples}\n",
    "    )\n",
    ")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda8be1",
   "metadata": {},
   "source": [
    "###### Using S3 as source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucket = args['bucket']\n",
    "train_input_key = args[\"train-source-key\"]\n",
    "test_input_key = args[\"test-source-key\"]\n",
    "\n",
    "\n",
    "train_dyF = glueContext.create_dynamic_frame.from_options(\n",
    "        's3',\n",
    "        {'paths': [f\"s3://{bucket}/{train_input_key}\"]},\n",
    "        'csv',\n",
    "        {'withHeader': True})\n",
    "test_dyF = glueContext.create_dynamic_frame.from_options(\n",
    "        's3',\n",
    "        {'paths': [f\"s3://{bucket}/{test_input_key}\"]},\n",
    "        'csv',\n",
    "        {'withHeader': True})\n",
    "\n",
    "\n",
    "Union_node_dyf = sparkUnion(\n",
    "    glueContext,\n",
    "    unionType=\"ALL\",\n",
    "    mapping={\n",
    "        \"source1\": train_dyF,\n",
    "        \"source2\": test_dyF,\n",
    "    },\n",
    "    transformation_ctx=\"Union_train_test\",\n",
    ")\n",
    "\n",
    "\n",
    "Union_node_dyf.count()\n",
    "\n",
    "\n",
    "\n",
    "mappings=[\n",
    "        (\"trans_date_trans_time\", \"string\", \"trans_date_trans_time\", \"timestamp\"),\n",
    "        (\"cc_num\", \"string\", \"cc_num\", \"bigint\"),\n",
    "        (\"merchant\", \"string\", \"merchant\", \"string\"),\n",
    "        (\"category\", \"string\", \"category\", \"string\"),\n",
    "        (\"amt\", \"string\", \"amt\", \"float\"),\n",
    "        (\"first\", \"string\", \"first\", \"string\"),\n",
    "        (\"late\", \"string\", \"late\", \"string\"),\n",
    "        (\"gender\", \"string\", \"gender\", \"string\"),\n",
    "        (\"street\", \"string\", \"street\", \"string\"),\n",
    "        (\"city\", \"string\", \"city\", \"string\"),\n",
    "        (\"state\", \"string\", \"state\", \"string\"),\n",
    "        (\"zip\", \"string\", \"zip\", \"int\"),\n",
    "        (\"lat\", \"string\", \"lat\", \"float\"),\n",
    "        (\"long\", \"string\", \"long\", \"float\"),\n",
    "        (\"city_pop\", \"string\", \"city_pop\", \"int\"),\n",
    "        (\"job\", \"string\", \"job\", \"string\"),\n",
    "        (\"dob\", \"string\", \"dob\", \"date\"),\n",
    "        (\"trans_num\", \"string\", \"trans_num\", \"string\"),\n",
    "        (\"unix_time\", \"string\", \"unix_time\", \"int\"),\n",
    "        (\"merch_lat\", \"string\", \"merch_lat\", \"float\"),\n",
    "        (\"merch_long\", \"string\", \"merch_long\", \"float\"),\n",
    "        (\"is_fraud\", \"string\", \"is_fraud\", \"binary\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834636e",
   "metadata": {},
   "source": [
    "###### Using glue data catalog as source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76243770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852394"
     ]
    }
   ],
   "source": [
    "\n",
    "# comment out this if uncommenting out the code above which reads from S3 as source\n",
    "Union_node_dyf = glueContext.create_dynamic_frame_from_catalog(\n",
    "    database = args[\"catalog_db\"],\n",
    "    table_name = args[\"catalog_table\"],\n",
    "    transformation_ctx = \"Read fraud train and test combined data from catalog table \")\n",
    "\n",
    "# This mapping is customised for catalog table inferred schema.\n",
    "# comment this out if uncommenting out the code above which reads from S3 as source\n",
    "mappings = [\n",
    "    (\"trans_date_trans_time\", \"string\", \"trans_date_trans_time\", \"timestamp\"),\n",
    "    (\"cc_num\", \"long\", \"cc_num\", \"long\"),\n",
    "    (\"merchant\", \"string\", \"merchant\", \"string\"),\n",
    "    (\"category\", \"string\", \"category\", \"string\"),\n",
    "    (\"amt\", \"double\", \"amt\", \"double\"),\n",
    "    (\"first\", \"string\", \"first\", \"string\"),\n",
    "    (\"last\", \"string\", \"last\", \"string\"),\n",
    "    (\"gender\", \"string\", \"gender\", \"string\"),\n",
    "    (\"street\", \"string\", \"street\", \"string\"),\n",
    "    (\"city\", \"string\", \"city\", \"string\"),\n",
    "    (\"state\", \"string\", \"state\", \"string\"),\n",
    "    (\"zip\", \"long\", \"zip\", \"long\"),\n",
    "    (\"lat\", \"double\", \"lat\", \"double\"),\n",
    "    (\"long\", \"double\", \"long\", \"double\"),\n",
    "    (\"city_pop\", \"long\", \"city_pop\", \"int\"),\n",
    "    (\"job\", \"string\", \"job\", \"string\"),\n",
    "    (\"dob\", \"string\", \"dob\", \"date\"),\n",
    "    (\"trans_num\", \"string\", \"trans_num\", \"string\"),\n",
    "    (\"unix_time\", \"long\", \"unix_time\", \"int\"),\n",
    "    (\"merch_lat\", \"double\", \"merch_lat\", \"double\"),\n",
    "    (\"merch_long\", \"double\", \"merch_long\", \"double\"),\n",
    "    (\"is_fraud\", \"long\", \"is_fraud\", \"short\"), #seems to drop all rows if casting to binary so use short\n",
    "]\n",
    "\n",
    "Union_node_dyf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12400a9",
   "metadata": {},
   "source": [
    "##### ApplyMapping \n",
    "\n",
    "This uses the glue applymapping transform on the DynamicDataframe to rename some columns and cast them to different types as per the mapping list of tuples defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56334442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- trans_date_trans_time: timestamp\n",
      "|-- cc_num: long\n",
      "|-- merchant: string\n",
      "|-- category: string\n",
      "|-- amt: double\n",
      "|-- first: string\n",
      "|-- last: string\n",
      "|-- gender: string\n",
      "|-- street: string\n",
      "|-- city: string\n",
      "|-- state: string\n",
      "|-- zip: long\n",
      "|-- lat: double\n",
      "|-- long: double\n",
      "|-- city_pop: int\n",
      "|-- job: string\n",
      "|-- dob: date\n",
      "|-- trans_num: string\n",
      "|-- unix_time: int\n",
      "|-- merch_lat: double\n",
      "|-- merch_long: double\n",
      "|-- is_fraud: short\n",
      "\n",
      "1852394"
     ]
    }
   ],
   "source": [
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_dyf = ApplyMapping.apply(\n",
    "    frame=Union_node_dyf,\n",
    "    mappings=mappings,\n",
    "    transformation_ctx=\"ApplyMapping\",\n",
    ")\n",
    "ApplyMapping_dyf.printSchema()\n",
    "\n",
    "ApplyMapping_dyf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773bb37",
   "metadata": {},
   "source": [
    "##### DropFields Transform \n",
    "\n",
    "Dropping fields which only add to noise in model - after checking mdoel variable importance plot after first run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf9e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- trans_date_trans_time: timestamp\n",
      "|-- cc_num: long\n",
      "|-- merchant: string\n",
      "|-- category: string\n",
      "|-- amt: double\n",
      "|-- first: string\n",
      "|-- last: string\n",
      "|-- gender: string\n",
      "|-- street: string\n",
      "|-- city: string\n",
      "|-- state: string\n",
      "|-- zip: long\n",
      "|-- city_pop: int\n",
      "|-- job: string\n",
      "|-- trans_num: string\n",
      "|-- is_fraud: short"
     ]
    }
   ],
   "source": [
    "\n",
    "# Script generated for node Drop Fields\n",
    "DropFields_dyf = DropFields.apply(\n",
    "    frame=ApplyMapping_dyf,\n",
    "    paths=['col0', \"merch_lat\",  \"merch_long\",\"lat\",\"long\",\"unix_time\", \"dob\"],\n",
    "    transformation_ctx=\"DropFields\",\n",
    ")\n",
    "\n",
    "DropFields_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4568e3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852394"
     ]
    }
   ],
   "source": [
    "DropFields_dyf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894dd5d",
   "metadata": {},
   "source": [
    "##### RenameField Transform\n",
    "\n",
    "Rename fileds corresponding to timestamp and label to their required names as understood by AWS Fraud Detector online insights model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b37097b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- cc_num: long\n",
      "|-- merchant: string\n",
      "|-- category: string\n",
      "|-- amt: double\n",
      "|-- first: string\n",
      "|-- last: string\n",
      "|-- gender: string\n",
      "|-- street: string\n",
      "|-- city: string\n",
      "|-- state: string\n",
      "|-- zip: long\n",
      "|-- city_pop: int\n",
      "|-- job: string\n",
      "|-- trans_num: string\n",
      "|-- EVENT_TIMESTAMP: timestamp\n",
      "|-- EVENT_LABEL: short"
     ]
    }
   ],
   "source": [
    "# Script generated for node Rename Field\n",
    "RenameField_timestamp= RenameField.apply(\n",
    "    frame=DropFields_dyf,\n",
    "    old_name=\"trans_date_trans_time\",\n",
    "    new_name=\"EVENT_TIMESTAMP\",\n",
    "    transformation_ctx=\"RenameField_timestamp\",\n",
    ")\n",
    "\n",
    "# Script generated for node Rename Field\n",
    "RenameField_label= RenameField.apply(\n",
    "    frame=RenameField_timestamp,\n",
    "    old_name=\"is_fraud\",\n",
    "    new_name=\"EVENT_LABEL\",\n",
    "    transformation_ctx=\"RenameField_label\",\n",
    ")\n",
    "\n",
    "RenameField_label.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8458444",
   "metadata": {},
   "source": [
    "###### Filter timeframes\n",
    "\n",
    "Convert to pyspark dataframe and use pyspark methods to create the train and test datasest by using defined timestamp\n",
    "filter ranges. We also rename the 0,1 values in the EVENT_LABEL column to 'fraud' and 'legit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e734368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|EVENT_TIMESTAMP    |EVENT_LABEL|\n",
      "+-------------------+-----------+\n",
      "|2020-04-29 23:59:18|legit      |\n",
      "|2020-04-29 23:58:29|fraud      |\n",
      "|2020-04-29 23:58:25|legit      |\n",
      "|2020-04-29 23:58:21|legit      |\n",
      "|2020-04-29 23:57:55|legit      |\n",
      "|2020-04-29 23:57:55|legit      |\n",
      "|2020-04-29 23:57:48|legit      |\n",
      "|2020-04-29 23:57:47|legit      |\n",
      "|2020-04-29 23:56:49|fraud      |\n",
      "|2020-04-29 23:56:45|legit      |\n",
      "|2020-04-29 23:55:03|legit      |\n",
      "|2020-04-29 23:52:52|legit      |\n",
      "|2020-04-29 23:52:46|legit      |\n",
      "|2020-04-29 23:51:29|legit      |\n",
      "|2020-04-29 23:50:06|legit      |\n",
      "|2020-04-29 23:49:59|legit      |\n",
      "|2020-04-29 23:49:39|legit      |\n",
      "|2020-04-29 23:46:49|legit      |\n",
      "|2020-04-29 23:46:18|fraud      |\n",
      "|2020-04-29 23:44:47|legit      |\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = RenameField_label.toDF()\n",
    "\n",
    "train_df = df.filter(df.EVENT_TIMESTAMP <  \"2020-04-30 00:00:00\").withColumn(\"EVENT_LABEL\", when(df.EVENT_LABEL == '0',\"legit\").when(df.EVENT_LABEL == '1',\"fraud \"))\n",
    "test_df = df.filter(df.EVENT_TIMESTAMP >  \"2020-08-30 00:00:00\").withColumn(\"EVENT_LABEL\", when(df.EVENT_LABEL == '0',\"legit\").when(df.EVENT_LABEL == '1',\"fraud \"))\n",
    "\n",
    "train_df.select(col('EVENT_TIMESTAMP'), col('EVENT_LABEL')).orderBy(desc('EVENT_TIMESTAMP')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67688e92",
   "metadata": {},
   "source": [
    "##### Create batch sample from test dataframe\n",
    "\n",
    "We convert the pyspark dataframes created earlier into DynamicDataFrames and use the sparkSqlQuery class to the run sql query (defined in the first block of this notebook), to take a random sample of the the data such that \n",
    "we have x number of fraud samples and y number of legit samples. We do not need the entire dataset for batch predictions to keep billing cost down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf0c10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dyf = DynamicFrame.fromDF(test_df, glueContext, \"test_dyf\")\n",
    "train_dyf = DynamicFrame.fromDF(train_df, glueContext, \"train_dyf\")\n",
    "\n",
    "sampled_test_dyf = sparkSqlQuery(glueContext, query = SqlQuery0, mapping = {\"myDataSource\":test_dyf}, transformation_ctx = \"SQLQuery_test_sample\")\n",
    "sampled_test_dyf.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d1bc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+------+---------+---------+------+--------------------+-----------------+-----+-----+--------+--------------------+--------------------+-------------------+-----------+\n",
      "|             cc_num|            merchant|     category|   amt|    first|     last|gender|              street|             city|state|  zip|city_pop|                 job|           trans_num|    EVENT_TIMESTAMP|EVENT_LABEL|\n",
      "+-------------------+--------------------+-------------+------+---------+---------+------+--------------------+-----------------+-----+-----+--------+--------------------+--------------------+-------------------+-----------+\n",
      "|     36890292963032|     fraud_Rau-Grant|    kids_pets|  7.04| Kimberly|     Webb|     F|  380 Martin Mission|           Girard|   GA|30426|    1100|Conservator, muse...|985bea2e56a683feb...|2020-09-28 19:51:36|      legit|\n",
      "|   2229378226512508| fraud_Schmeler-Howe|personal_care| 81.68|   Thomas|     Hale|     M|   949 Smith Parkway|        Cazenovia|   WI|53924|    1360|Occupational hygi...|58a41bff00a915e87...|2020-12-13 18:40:55|      legit|\n",
      "|4018105808392773675|  fraud_Bogisich Inc|  grocery_pos|196.71|Katherine|     Love|     F|5884 Sandoval Squ...|       Allenhurst|   NJ| 7711|    1533|Administrator, ch...|db4ac74f553c09243...|2020-10-11 07:04:05|      legit|\n",
      "|     30238755902988|   fraud_Schmitt Ltd|     misc_net|  3.53| Danielle|       Yu|     F|5395 Colon Burgs ...|           Thrall|   TX|76578|    1766|           Press sub|a223044da2a0c8702...|2020-09-11 14:24:36|      legit|\n",
      "|   4777065439639721|    fraud_Little Ltd|    kids_pets| 38.71|    Peter| Caldwell|     M|08966 Beltran Rou...|             Oaks|   PA|19456|     737|       Oceanographer|cb2c5cfb0e2a7bcd2...|2020-10-27 13:46:50|      legit|\n",
      "|   6011360759745864|   fraud_Yost-Rogahn|personal_care|  1.37|   Steven| Williams|     M|231 Flores Pass S...|         Edinburg|   VA|22824|    6018|Designer, multimedia|437d38f389ab53723...|2020-10-20 22:22:09|      legit|\n",
      "|   3567527758368741|fraud_White and Sons|         home| 40.73|   Amanda|    Vance|     F|14601 Downs Skywa...|    Sterling City|   TX|76951|    1143|Scientist, biomed...|8365a97c00b6c360a...|2020-08-30 19:46:42|      legit|\n",
      "|   6011581063717667|     fraud_Emard Inc|gas_transport| 50.47|    Jerry|  Perkins|     M|3867 Susan Corner...|         Brashear|   MO|63533|     805|Private music tea...|48359015c593cf8de...|2020-12-23 09:54:50|      legit|\n",
      "|     30273037698427|  fraud_Luettgen PLC|gas_transport|  27.7|   Andrew|Patterson|     M|06959 Stephen Bra...|            Thida|   AR|72165|     111|Careers informati...|0ae3377b0bfea90bf...|2020-10-27 06:08:13|      legit|\n",
      "|4181833256558613886|fraud_Olson, Beck...|gas_transport| 41.61|  Jessica|   Potter|     F|7600 Stephen Cour...|        Red River|   NM|87558|     606|Surveyor, land/ge...|8ed1fd5de5fe08c9a...|2020-12-18 07:04:23|      legit|\n",
      "|    376028110684021|fraud_Corwin-Collins|gas_transport| 72.09|    Aaron|   Murray|     M|624 Hale Springs ...|        Meadville|   MO|64659|     964|Tourist informati...|5f747b02008c143ab...|2020-10-27 01:00:19|      legit|\n",
      "|   3568736585751727|     fraud_Rau-Robel|    kids_pets| 53.76|   Thomas|    Cross|     M|7566 Thompson Cre...|          Elkhart|   IA|50073|    1195|Trading standards...|4032aefaaa6420f12...|2020-11-10 19:53:09|      legit|\n",
      "|   3543299015720986|fraud_Stamm-Rodri...|     misc_pos| 28.12|   Angela|    White|     F|       137 Adam Dale|          Lebanon|   VA|24266|    9521|Teacher, English ...|74db747636e2311c7...|2020-09-14 10:22:43|      legit|\n",
      "|       676372984911|fraud_Stiedemann Ltd|  food_dining|  2.64|    Vicki|  Mendoza|     F|3645 Atkins Islan...|            Esbon|   KS|66941|     242|     Tourism officer|ea494548e5f840643...|2020-12-23 15:53:36|      legit|\n",
      "|   5104807169438134|     fraud_Kuhic Inc|  grocery_pos|174.87|   Regina| Johnston|     F|641 Steven Mall A...|Westhampton Beach|   NY|11978|    3255|    Financial trader|38614d53817f671f6...|2020-10-30 07:58:45|      legit|\n",
      "|      4471568287204|fraud_Schaefer, F...|entertainment|114.46|   Dakota|Maldonado|     M|  369 Cochran Radial|           Pelham|   NC|27311|    3402|Insurance underwr...|33ddd258e60a18be2...|2020-09-01 04:01:23|      legit|\n",
      "|        60495593109|fraud_Baumbach, S...| shopping_pos|  6.53|  Randall|   Dillon|     M|4440 George Mills...|           Dallas|   TX|75210| 1263321|Television camera...|6f478f69d3153bf59...|2020-12-15 02:19:00|      legit|\n",
      "|   4048508450704760|fraud_McGlynn-Jas...|     misc_pos|272.15|   Cheryl|   Melton|     F|5053 Bell Crescen...|        Fullerton|   NE|68638|    1749|    Surveyor, mining|4d6e1ac25ad7df330...|2020-09-14 12:37:29|      legit|\n",
      "|4355790796238264643|fraud_Bartoletti-...|gas_transport| 59.22|   Tanner|    Davis|     M|2632 Stevens Ligh...|           Payson|   IL|62360|    1656| Exhibition designer|67882f0d553999a02...|2020-12-25 10:13:00|      legit|\n",
      "|   4277232699798846|fraud_Hackett-Lue...|  grocery_pos|133.86| Jennifer|    Vance|     F|13299 Patrick Ter...|            Rhame|   ND|58651|     475|         Illustrator|5dcade7f02b801ee1...|2020-12-05 02:48:27|      legit|\n",
      "+-------------------+--------------------+-------------+------+---------+---------+------+--------------------+-----------------+-----+-----+--------+--------------------+--------------------+-------------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sampled_test_dyf.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f50f9",
   "metadata": {},
   "source": [
    "##### Repartiton\n",
    "\n",
    "Reparition the dynamicdataframe to single partition dataframe to avoid saving multiple paritions in s3 bucket. We would need to convert to pyspark dataframe to use the 'repartition' method and then convert back to Dynamic DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e8cd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "single_part_train_dyf = DynamicFrame.fromDF(train_dyf.toDF().repartition(1), glueContext, \"single_partition_train\")\n",
    "single_part_test_dyf = DynamicFrame.fromDF(sampled_test_dyf.toDF().repartition(1), glueContext, \"single_partition_test_sample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc860c",
   "metadata": {},
   "source": [
    "#### Saving Train and sampled batch data to S3 \n",
    "\n",
    "The next code blocks save the train and sampled test dataset to path in S3.  By default `glueContext.write_dynamic_frame.from_options` saves the file with a random name.\n",
    "The code in `write_output_to_s3` renames the file to something sensible and then deletes the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb8214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving training data ......\n",
      "saving dyanmic frame to S3 bucket with prefix path: input\n",
      "renaming filename to input/fraudTrain_glue_transformed.csv as glue output filename is random\n",
      "deleting original output input/run-1656278830255-part-r-00000...."
     ]
    }
   ],
   "source": [
    "\n",
    "train_dest_split = args[\"train-dest-key\"].split('/')\n",
    "train_filename = train_dest_split.pop(-1)\n",
    "renamed_key = args['train-dest-key']\n",
    "transformation_ctx = \"S3bucket_write_train_dyf\"\n",
    "prefix = '/'.join(train_dest_split)\n",
    "s3_path = os.path.join(\"s3://\", args[\"bucket\"], prefix)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Saving training data ......\")\n",
    "write_output_to_s3(train_dyf, s3_path, prefix, renamed_key, transformation_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84f061d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test data ......\n",
      "saving dyanmic frame to S3 bucket with prefix path: input\n",
      "renaming filename to input/fraudTest_glue_transformed.csv as glue output filename is random\n",
      "deleting original output input/run-1656278830255-part-r-00001...."
     ]
    }
   ],
   "source": [
    "test_dest_split = args[\"test-dest-key\"].split('/')\n",
    "test_filename = test_dest_split.pop(-1)\n",
    "renamed_key = args['test-dest-key']\n",
    "transformation_ctx = \"S3bucket_write_test_dyf\"\n",
    "prefix = '/'.join(test_dest_split)\n",
    "\n",
    "print(\"Saving test data ......\")\n",
    "write_output_to_s3(sampled_test_dyf, s3_path, prefix, renamed_key, transformation_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe7218e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#uncomment this when running job in glue\n",
    "#job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826e687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
